import datetime
import typing

import sqlalchemy
import logging
from typing import Optional, List

from sqlalchemy import Index, update
from sqlalchemy.exc import ProgrammingError
from sqlmodel import SQLModel, create_engine, Session, select
from sqlmodel.sql.expression import SelectOfScalar

import isb_lib
from isb_lib.models.person import Person
from isb_lib.models.thing import Thing, ThingIdentifier, Point
from isb_web.schemas import ThingPage


DRAFT_RESOLVED_URL = "DRAFT"
DRAFT_AUTHORITY_ID = "DRAFT"
DRAFT_RESOLVED_STATUS = -1


class SQLModelDAO:
    def __init__(self, db_url: Optional[str], echo: bool = False):
        # This is a strange initializer, but FastAPI wants us to construct the object before we know we
        # want to use it.  So, separate out the object construction from the database connection.
        # In unit tests, this ends up getting swapped out and unused, which is the source of the confusion.
        if db_url is not None:
            self.connect_sqlmodel(db_url, echo)
        else:
            self.engine = None

    # Utility method to attempt to create an index but catch an exception if it already exists
    def _create_index(self, index: Index):
        try:
            index.create(self.engine)
        except ProgrammingError:
            pass

    def connect_sqlmodel(self, db_url: str, echo: bool = False):
        self.engine = create_engine(db_url, echo=echo)
        SQLModel.metadata.create_all(self.engine)
        # There doesn't appear to be a SQLModel-native way of creating those, so fall back to SQLAlchemy
        id_resolved_status_authority_id_idx = Index(
            "_id_resolved_status_authority_id_idx",
            Thing.primary_key,
            Thing.resolved_status,
            Thing.authority_id,
        )
        # These index creations will throw if they already exist -- there's nothing to do in that case
        self._create_index(id_resolved_status_authority_id_idx)

        authority_id_tcreated_idx = Index(
            "authority_id_tcreated_idx", Thing.authority_id, Thing.tcreated
        )
        self._create_index(authority_id_tcreated_idx)

        item_type_status_idx = Index(
            "item_type_status_idx", Thing.item_type, Thing.resolved_status
        )
        self._create_index(item_type_status_idx)

        resolved_status_authority_id_idx = Index(
            "resolved_status_authority_id_idx",
            Thing.resolved_status,
            Thing.authority_id,
        )
        self._create_index(resolved_status_authority_id_idx)

        # guid_thing_id_idx = Index(
        #     "guid_thing_id_idx", ThingIdentifier.guid, ThingIdentifier.thing_id
        # )
        # self._create_index(guid_thing_id_idx)

    def get_session(self) -> Session:
        return Session(self.engine)


def read_things_summary(
    session: Session,
    offset: int,
    limit: int = 100,
    status: int = 200,
    authority: Optional[str] = None,
) -> tuple[int, int, List[ThingPage]]:
    # Fetch summary records of Things (but not the full content), suitable for paging in an API
    count_statement = session.query(Thing)
    if authority is not None:
        count_statement = count_statement.filter(Thing.authority_id == authority)
    count_statement = count_statement.filter(Thing.resolved_status == status)
    overall_count = count_statement.count()
    if limit > 0:
        overall_pages = overall_count / limit
    else:
        overall_pages = 0
    things_statement = select(
        Thing.primary_key,
        Thing.id,
        Thing.authority_id,
        Thing.tcreated,
        Thing.resolved_status,
        Thing.resolved_url,
        Thing.resolve_elapsed,
    )
    if authority is not None:
        things_statement = things_statement.filter(Thing.authority_id == authority)
    things_statement = things_statement.filter(Thing.resolved_status == status)
    if offset > 0:
        things_statement = things_statement.offset(offset)
    if limit > 0:
        things_statement = things_statement.limit(limit)
    things_results = session.exec(things_statement)
    return overall_count, overall_pages, things_results.all()


def get_thing_with_id(session: Session, identifier: str) -> Optional[Thing]:
    statement = (
        select(Thing).filter(Thing.id == identifier).order_by(Thing.primary_key.asc())
    )
    result = session.exec(statement).first()
    if result is None:
        # Fall back to querying the Identifiers table
        identifiers_statement = select(Thing).where(
            Thing.identifiers.like(f"%{identifier}%")
        )
        result = session.exec(identifiers_statement).first()
    return result


def get_things_with_ids(session: Session, identifiers: list[str]) -> list[Thing]:
    statement = select(Thing).where(Thing.id.in_(identifiers))
    things = session.exec(statement).all()
    return things


def get_thing_identifiers_for_thing(session: Session, thing_id: int) -> list[str]:
    statement = select(Thing.identifiers).where(Thing.primary_key == thing_id)
    session_exec = session.exec(statement)
    allrows = session_exec.all()
    thing_identifiers = []
    for row in allrows:
        thing_identifiers.append(row[0])
    return thing_identifiers


def last_time_thing_created(
    session: Session, authority_id: str
) -> Optional[datetime.datetime]:
    # A bit of a hack to work around postgres perf issues.  Limit the number of records to examine by including a
    # time created date in the qualifier.
    one_year_ago = datetime.datetime(
        year=datetime.date.today().year - 1, month=1, day=1
    )
    created_select = select(Thing.tcreated)
    if authority_id is not None:
        created_select = created_select.filter(Thing.authority_id == authority_id)
    created_select = (
        created_select.filter(Thing.tcreated >= one_year_ago)
        .limit(1)
        .order_by(Thing.tcreated.desc())
    )
    result = session.exec(created_select).first()
    return result


def _base_thing_select(
    authority: Optional[str] = None,
    status: int = 200,
    limit: int = 100,
    offset: int = 0,
    min_id: int = 0,
) -> SelectOfScalar[Thing]:
    thing_select = select(Thing).filter(Thing.resolved_status == status)
    if authority is not None:
        thing_select = thing_select.filter(Thing.authority_id == authority)
    if offset > 0:
        thing_select = thing_select.offset(offset)
    if limit > 0:
        thing_select = thing_select.limit(limit)
    if min_id > 0:
        thing_select = thing_select.filter(Thing.primary_key > min_id)
    return thing_select


def paged_things_with_ids(
    session: Session,
    authority: Optional[str] = None,
    status: int = 200,
    limit: int = 100,
    offset: int = 0,
    min_time_created: Optional[datetime.datetime] = None,
    min_id: int = 0,
) -> List[Thing]:
    thing_select = _base_thing_select(authority, status, limit, offset, min_id)

    if min_time_created is not None:
        thing_select = thing_select.filter(Thing.tcreated >= min_time_created)
    thing_select = thing_select.order_by(Thing.primary_key.asc())
    return session.exec(thing_select).all()


def things_for_sitemap(
    session: Session,
    authority: Optional[str] = None,
    status: int = 200,
    limit: int = 100,
    offset: int = 0,
    min_tstamp: Optional[datetime.datetime] = None,
    min_id: int = 0,
) -> List[Thing]:
    """Returns a list of Things suitable for generating a sitemap.

    In order to allow older unchanged records to not be refetched, order by the timestamp to support automatic diffing
    """
    thing_select = _base_thing_select(authority, status, limit, offset, min_id)
    if min_tstamp is not None:
        thing_select = thing_select.filter(Thing.tstamp >= min_tstamp)
    thing_select = thing_select.order_by(Thing.tstamp.asc(), Thing.primary_key.asc())
    return session.exec(thing_select).all()


def get_thing_meta(session: Session):
    dbq = session.query(
        sqlalchemy.sql.label("status", Thing.resolved_status),
        sqlalchemy.sql.label("count", sqlalchemy.func.count(Thing.resolved_status)),
    ).group_by(Thing.resolved_status)
    meta = {"status": dbq.all()}
    dbq = session.query(
        sqlalchemy.sql.label("authority", Thing.authority_id),
        sqlalchemy.sql.label("count", sqlalchemy.func.count(Thing.authority_id)),
    ).group_by(Thing.authority_id)
    meta["authority"] = dbq.all()
    return meta


def get_sample_types(session: Session):
    dbq = (
        session.query(
            sqlalchemy.sql.label("item_type", Thing.item_type),
            sqlalchemy.sql.label("count", sqlalchemy.func.count(Thing.item_type)),
        )
        .filter(Thing.resolved_status == 200)
        .group_by(Thing.item_type)
    )
    return dbq.all()


def geome_identifiers_from_resolved_content(resolved_content: typing.Dict) -> list[str]:
    children = resolved_content.get("children")
    identifiers = []
    if children is not None:
        for child in children:
            child_ark = child["bcid"]
            identifiers.append(child_ark)
    return identifiers


def _insert_geome_identifiers(thing: Thing):
    # For now, we will fail all requests for parent IDs, because events appear in multiple samples
    # and would violate referential integrity if we made pointers to children from the event ID
    # parent = thing.resolved_content.get("parent")
    # if parent is not None:
    #     event_ark = parent["bcid"]
    #     if event_ark in arks_to_bp:
    #         print()
    #     event_identifier = ThingIdentifier(guid=event_ark, thing_id=thing.primary_key)
    #     session.add(event_identifier)
    identifiers = geome_identifiers_from_resolved_content(thing.resolved_content)
    for identifier in identifiers:
        thing.insert_thing_identifier_if_not_present(identifier)


def opencontext_identifiers_from_resolved_content(resolved_content: typing.Dict) -> list[str]:
    identifiers = []
    citation_uri = resolved_content["citation uri"]
    if citation_uri is not None and type(citation_uri) is str:
        identifiers.append(isb_lib.normalized_id(citation_uri))
    return identifiers


def _insert_open_context_identifiers(thing: Thing):
    citation_uri = thing.resolved_content["citation uri"]
    if citation_uri is not None and type(citation_uri) is str:
        open_context_uri = isb_lib.normalized_id(citation_uri)
        thing.insert_thing_identifier_if_not_present(open_context_uri)


def _standard_identifier(thing: Thing) -> str:
    return thing.id


def thing_identifiers_from_resolved_content(authority_id: str, resolved_content: typing.Dict) -> list[str]:
    identifiers = []
    if authority_id == "GEOME":
        identifiers += geome_identifiers_from_resolved_content(resolved_content)
    elif authority_id == "OPENCONTEXT":
        identifiers += opencontext_identifiers_from_resolved_content(resolved_content)
    return identifiers


def insert_identifiers(thing: Thing):
    identifiers = thing_identifiers_from_resolved_content(thing.authority_id, thing.resolved_content)
    identifiers.append(_standard_identifier(thing))
    for identifier in identifiers:
        thing.insert_thing_identifier_if_not_present(identifier)


def save_thing(session: Session, thing: Thing):
    insert_identifiers(thing)
    logging.debug("Going to add thing to session")
    session.add(thing)
    logging.debug("Added thing to session")
    session.commit()
    logging.debug("committed session")


def save_draft_thing_with_id(session: Session, draft_id: str) -> Thing:
    draft_thing = Thing()
    draft_thing.id = draft_id
    draft_thing.resolved_status = DRAFT_RESOLVED_STATUS
    draft_thing.authority_id = DRAFT_AUTHORITY_ID
    draft_thing.resolved_url = DRAFT_RESOLVED_URL
    draft_thing.tcreated = datetime.datetime.now()
    save_thing(session, draft_thing)
    return draft_thing


def save_or_update_thing(session: Session, thing: Thing):
    try:
        save_thing(session, thing)
    except sqlalchemy.exc.IntegrityError as e:
        session.rollback()
        logging.info(f"Thing already exists {thing.id}, will recreate record")
        logging.info(
            f"Thing already exists with primary key {thing.primary_key}, will update record"
        )
        existing_thing = get_thing_with_id(session, thing.id)
        thing_identifiers = (
            []
        )  # get_thing_identifiers_for_thing(session, existing_thing.primary_key)
        existing_thing.identifiers = thing_identifiers
        if existing_thing is None:
            logging.error(
                f"Error attempting to save existing thing that doesn't exist: {thing.id}, exception: {e}"
            )
        else:
            existing_thing.take_values_from_other_thing(thing)
            try:
                save_thing(session, existing_thing)
            except sqlalchemy.exc.IntegrityError as integrity_error:
                session.rollback()
                logging.error(
                    f"Got error attempting to save existing thing {thing.id}, exception: {integrity_error}"
                )


def all_thing_identifier_objects(
    session: Session, min_id: int, batch_size: int
) -> list[ThingIdentifier]:
    thing_identifiers_select = (
        select(ThingIdentifier)
        .filter(ThingIdentifier.thing_id >= min_id)
        .order_by(ThingIdentifier.thing_id.asc())
        .limit(batch_size)
    )
    thing_identifiers = session.exec(thing_identifiers_select).all()
    return thing_identifiers


def things_with_null_identifiers(session: Session) -> list[Thing]:
    # Note that SQLAlchemy needs things to be written this way but it triggers flake8 so manually override
    things_select = select(Thing).filter(Thing.identifiers == None)  # noqa: E711
    things = session.exec(things_select).all()
    return things


def all_thing_identifiers(session: Session, authority: typing.Optional[str] = None) -> typing.Dict[str, int]:
    thing_identifiers_select = select(Thing.primary_key, Thing.identifiers)
    if authority is not None:
        thing_identifiers_select = thing_identifiers_select.where(Thing.authority_id == authority)
    thing_identifiers = session.execute(thing_identifiers_select).fetchall()
    thing_identifiers_dict = {}
    for row in thing_identifiers:
        for identifier in row[1]:
            thing_identifiers_dict[identifier] = row[0]
    return thing_identifiers_dict


def all_thing_primary_keys(session: Session, authority: typing.Optional[str] = None) -> typing.Dict[str, int]:
    thing_pk_select = select(Thing.primary_key, Thing.id)
    if authority is not None:
        thing_pk_select = thing_pk_select.where(Thing.authority_id == authority)
    thing_pks = session.execute(thing_pk_select).fetchall()
    thing_pks_dict = {}
    for row in thing_pks:
        thing_pks_dict[row[1]] = row[0]
    return thing_pks_dict


def h3_values_without_points(session: Session, all_h3_values: set) -> list:
    h3_select = select(Point.h3).distinct().where(Point.h3.in_(list(all_h3_values)))
    h3_with_points_set = set(session.exec(h3_select).all())
    return all_h3_values - h3_with_points_set


def h3_to_height(session: Session) -> typing.Dict:
    h3_select = select(Point.h3, Point.height).filter(Point.height != None)  # noqa: E711
    point_rows = session.execute(h3_select).fetchall()
    point_dict = {}
    for row in point_rows:
        point_dict[row[0]] = row[1]
    return point_dict


def mark_thing_not_found(session: Session, thing_id: str, resolved_url: str):
    """In case we get an error fetching a thing, mark it as not found in the database"""
    existing_thing = get_thing_with_id(session, thing_id)
    if existing_thing is None:
        thing = Thing()
        thing.id = thing_id
        thing.resolved_status = 404
        thing.tresolved = datetime.datetime.now()
        thing.resolved_url = resolved_url
        save_thing(session, thing)
    else:
        session.execute(
            update(isb_lib.models.thing.Thing)
            .where(isb_lib.models.thing.Thing.id == thing_id)
            .values(resolved_status=404)
            .values(resolved_url=resolved_url)
        )


def save_person_with_orcid_id(session: Session, orcid_id: str) -> Person:
    person = Person()
    person.orcid_id = orcid_id
    session.add(person)
    session.commit()
    return person


def all_orcid_ids(session: Session) -> list[str]:
    orcid_ids_select = select(Person.orcid_id)
    orcid_id_rows = session.execute(orcid_ids_select).fetchall()
    orcid_ids = []
    for row in orcid_id_rows:
        orcid_ids.append(row[0])
    return orcid_ids


THING_EXPORT_FIELD_LIST = "id, tstamp, tcreated, item_type, authority_id, resolved_url, resolved_status, tresolved, resolve_elapsed, resolved_content, resolved_media_type, _id, identifiers, h3"


def dump_things_with_ids_to_file(session: Session, identifiers: list[str], file_path: str):
    quoted_identifiers = [f"'{identifier}'" for identifier in identifiers]
    sql = f"copy (select {THING_EXPORT_FIELD_LIST} from thing where id in ({','.join(quoted_identifiers)})) to '{file_path}'"
    session.execute(sql)


def load_things_from_file(session: Session, file_path: str):
    sql = f"copy thing ({THING_EXPORT_FIELD_LIST}) from '{file_path}'"
    session.execute(sql)
    session.commit()
